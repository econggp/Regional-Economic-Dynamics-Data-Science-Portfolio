# -*- coding: utf-8 -*-
"""índices_censos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sGPNTW_P2kMcRrVNFTUNJZ3fYiMabET0
"""

import pandas as pd
import numpy as np

df = pd.read_excel('/content/SAIC_Exporta_202623_11343700.xlsx')

def validar_datos(df, columnas_criticas):
    """Valida que no haya valores críticos faltantes"""
    reporte = {}
    for col in columnas_criticas:
        nulos = df[col].isna().sum()
        ceros = (df[col] == 0).sum()
        reporte[col] = {'nulos': nulos, 'ceros': ceros}
    return pd.DataFrame(reporte)

# Ejecutar validación
columnas_criticas = ['pot', 'pacd', 'ppvs', 'tga', 'ataf', 'vacb']
print(validar_datos(df, columnas_criticas))

# Columnas que son etiquetas (ID)
columnas_identidad = ['tcode', 'tent', 'NOMGEO', 'AE', 'ID']

variables = df.columns.difference(columnas_identidad)

# Limpieza a todas las variables numéricas simultáneamente
df[variables] = df[variables].apply(pd.to_numeric, errors='coerce')

def calcular_totales_groupby(df, columnas, grupo, sufijo):
    """
    Calcula totales de múltiples columnas en una sola operación groupby.

    Parámetros:
    -----------
    df : DataFrame
        DataFrame de trabajo
    columnas : list
        Lista de columnas a agregar
    grupo : list
        Columnas para agrupar
    sufijo : str
        Sufijo para las columnas resultantes (ej: '_i', '_j', '_n')

    Retorna:
    --------
    DataFrame con las columnas agregadas
    """
    totales = df.groupby(grupo)[columnas].transform('sum')
    totales.columns = [f'{col}{sufijo}' for col in totales.columns]
    return totales

# COLUMNAS MÉTRICAS
metricas = ['pot', 'pacd', 'ppvs', 'UE', 'vacb', 'itot', 'ataf', 'fbcf']

# TOTALES POR SECTOR (sufijo _i)
df_totales_i = calcular_totales_groupby(df, metricas, ['tcode', 'AE'], '_i')

# TOTALES POR ENTIDAD (sufijo _j)
df_totales_j = calcular_totales_groupby(df, metricas, ['tcode', 'NOMGEO'], '_j')

# TOTALES NACIONALES (sufijo _tn)
df_totales_n = calcular_totales_groupby(df, metricas, ['tcode'], '_tn')

# Identificar y eliminar columnas duplicadas antes de concatenar para evitar el error
# debido a columnas con nombres repetidos
existing_calculated_cols = [col for col in df.columns if col.endswith(('_i', '_j', '_tn'))]
df = df.drop(columns=existing_calculated_cols, errors='ignore')

# Unir todas las nuevas columnas en una sola operación para evitar la fragmentación
df = pd.concat([df, df_totales_i, df_totales_j, df_totales_n], axis=1)

# VALIDACIÓN RÁPIDA
print(f"Columnas creadas: {len(df_totales_i.columns) + len(df_totales_j.columns) + len(df_totales_n.columns)}")
print(f"Valores NaN en totales: {df[df_totales_i.columns].isna().sum().sum()}")

# FUNCIÓN AUXILIAR PARA DIVISIÓN SEGURA
def safe_divide(numerator, denominator, default=0):
    return np.where(denominator != 0, numerator / denominator, default)

"""Índices de capacidades"""

# automa
df['automa'] = safe_divide(df['atmep'], df['ppvs'])

# ecpacd
df['ecpacd'] = safe_divide(df['tspacd'], (df['tga'] - df['tspacd']))

# ecppvs
df['ecppvs'] = safe_divide(df['tsppvs'], (df['tga'] - df['tsppvs']))

# sact
df['sact'] = safe_divide((df['ataf'] - df['dtaf']), df['ataf'])

# ecos
df['ecos'] = safe_divide(df['vacb'], df['tga'])

# efene
df['efene'] = safe_divide((df['gcee'] + df['ccle']), df['vacb'])

# mbi
df['mbi'] = safe_divide((df['tin'] - df['tga']), df['tin'])

df['roi'] = safe_divide(df['vacb'], df['fbcf'])

# cdig
df['cdig'] = safe_divide(df['atecp'], df['pot'])

# intal
df['intal'] = safe_divide((df['cspct'] + df['tspacd']), df['tga'])

# ite
df['ite'] = safe_divide((df['atecp'] + df['atmep']), df['ataf'])

"""Índices de derrames"""

df['pn_pot'] = safe_divide(df['pot_i'], df['pot_tn'])
df['pn_pacd'] = safe_divide(df['pacd_i'], df['pacd_tn'])
df['pn_ppvs'] = safe_divide(df['ppvs_i'], df['ppvs_tn'])
df['pn_ue'] = safe_divide(df['UE_i'], df['UE_tn'])

df['marpot'] = safe_divide(df['pot'], safe_divide(df['pot_j'], df['pn_pot']))
df['marpacd'] = safe_divide(df['pacd'], safe_divide(df['pacd_j'], df['pn_pot']))
df['marppvs'] = safe_divide(df['ppvs'], safe_divide(df['ppvs_j'], df['pn_pot']))

df_2023 = df[df['tcode'] == 2023].copy()
df_2018 = df[df['tcode'] == 2018].copy()
df_2013 = df[df['tcode'] == 2013].copy()
df_2008 = df[df['tcode'] == 2008].copy()
df_2003 = df[df['tcode'] == 2003].copy()

top_5_sec23 = df_2023.sort_values(['NOMGEO', 'marpot'], ascending=[True, False])
top_5_sec23 = top_5_sec23.groupby('NOMGEO').head(5)
top_5_sec18 = df_2018.sort_values(['NOMGEO', 'marpot'], ascending=[True, False])
top_5_sec18 = top_5_sec18.groupby('NOMGEO').head(5)
top_5_sec13 = df_2013.sort_values(['NOMGEO', 'marpot'], ascending=[True, False])
top_5_sec13 = top_5_sec13.groupby('NOMGEO').head(5)
top_5_sec08 = df_2008.sort_values(['NOMGEO', 'marpot'], ascending=[True, False])
top_5_sec08 = top_5_sec08.groupby('NOMGEO').head(5)
top_5_sec03 = df_2003.sort_values(['NOMGEO', 'marpot'], ascending=[True, False])
top_5_sec03 = top_5_sec03.groupby('NOMGEO').head(5)

resultado_top23 = top_5_sec23[['NOMGEO', 'AE', 'marpot']]
resultado_top18 = top_5_sec18[['NOMGEO', 'AE', 'marpot']]
resultado_top13 = top_5_sec13[['NOMGEO', 'AE', 'marpot']]
resultado_top08 = top_5_sec08[['NOMGEO', 'AE', 'marpot']]
resultado_top03 = top_5_sec03[['NOMGEO', 'AE', 'marpot']]

# Ejemplo
print(resultado_top23[resultado_top23['NOMGEO'].str.contains("Nuevo León", na=False)])
print(resultado_top18[resultado_top18['NOMGEO'].str.contains("Nuevo León", na=False)])
print(resultado_top13[resultado_top13['NOMGEO'].str.contains("Nuevo León", na=False)])
print(resultado_top08[resultado_top08['NOMGEO'].str.contains("Nuevo León", na=False)])
print(resultado_top03[resultado_top03['NOMGEO'].str.contains("Nuevo León", na=False)])

# Proporciones regionales
df['ps_pot'] = safe_divide(df['pot'], df['pot_j'])
df['ps_pacd'] = safe_divide(df['pacd'], df['pacd_j'])
df['ps_ppvs'] = safe_divide(df['ppvs'], df['ppvs_j'])
df['ps_vacb'] = safe_divide(df['vacb'], df['vacb_j'])

# Calcular el componente de Shannon: p_i * ln(p_i)
df['ps_pot_safe'] = df['ps_pot'].clip(lower=1e-10)  # Evitar log(0)
df['sp_pot'] = df['ps_pot'] * np.log(df['ps_pot_safe'])

df['ps_pacd_safe'] = df['ps_pacd'].clip(lower=1e-10)  # Evitar log(0)
df['sp_pacd'] = df['ps_pacd'] * np.log(df['ps_pacd_safe'])

df['ps_ppvs_safe'] = df['ps_ppvs'].clip(lower=1e-10)  # Evitar log(0)
df['sp_ppvs'] = df['ps_ppvs'] * np.log(df['ps_ppvs_safe'])

# DIV
sh_pot = df.groupby(['tcode', 'NOMGEO']).agg(
    h_pot=('sp_pot', lambda x: x.sum() * -1),
    S_sectores=('AE', 'count')).reset_index()
sh_pacd = df.groupby(['tcode', 'NOMGEO']).agg(
    h_pacd=('sp_pacd', lambda x: x.sum() * -1),
    S_sectores=('AE', 'count')).reset_index()
sh_ppvs = df.groupby(['tcode', 'NOMGEO']).agg(
    h_ppvs=('sp_ppvs', lambda x: x.sum() * -1),
    S_sectores=('AE', 'count')).reset_index()

# Merge Shannon indices and sector count into df
df = pd.merge(df, sh_pot[['tcode', 'NOMGEO', 'h_pot', 'S_sectores']], on=['tcode', 'NOMGEO'], how='left')
df = pd.merge(df, sh_pacd[['tcode', 'NOMGEO', 'h_pacd']], on=['tcode', 'NOMGEO'], how='left')
df = pd.merge(df, sh_ppvs[['tcode', 'NOMGEO', 'h_ppvs']], on=['tcode', 'NOMGEO'], how='left')

# Equitabilidad Pielou (J')
sh_pot['ep_pot'] = sh_pot['h_pot'] / np.log(sh_pot['S_sectores'])
sh_pacd['ep_pacd'] = sh_pacd['h_pacd'] / np.log(sh_pacd['S_sectores'])
sh_ppvs['ep_ppvs'] = sh_ppvs['h_ppvs'] / np.log(sh_ppvs['S_sectores'])

# Merge Pielou's equitability into df
df = pd.merge(df, sh_pot[['tcode', 'NOMGEO', 'ep_pot']], on=['tcode', 'NOMGEO'], how='left')
df = pd.merge(df, sh_pacd[['tcode', 'NOMGEO', 'ep_pacd']], on=['tcode', 'NOMGEO'], how='left')
df = pd.merge(df, sh_ppvs[['tcode', 'NOMGEO', 'ep_ppvs']], on=['tcode', 'NOMGEO'], how='left')

import seaborn as sns
import matplotlib.pyplot as plt

heatmap_data = sh_pot.pivot(index='NOMGEO', columns='tcode', values='ep_pot')

plt.figure(figsize=(12, 10))
sns.set_theme(style="white")

sns.heatmap(heatmap_data,
            annot=True,
            fmt=".2f",
            cmap="RdYlGn",
            linewidths=.5,
            cbar_kws={'label': 'Índice de equitabilidad (Pielou)'})

plt.title('Evolución de la diversidad económica por estado (2003-2023)', fontsize=15)
plt.xlabel('Año censal', fontsize=12)
plt.ylabel('Entidad federativa', fontsize=12)

plt.show()

heatmap_data = sh_pacd.pivot(index='NOMGEO', columns='tcode', values='ep_pacd')

plt.figure(figsize=(12, 10))
sns.set_theme(style="white")

sns.heatmap(heatmap_data,
            annot=True,
            fmt=".2f",
            cmap="RdYlGn",
            linewidths=.5,
            cbar_kws={'label': 'Índice de equitabilidad (Pielou)'})

plt.title('Evolución de la diversidad económica por estado PACD (2003-2023)', fontsize=15)
plt.xlabel('Año censal', fontsize=12)
plt.ylabel('Entidad federativa', fontsize=12)

plt.show()

heatmap_data = sh_ppvs.pivot(index='NOMGEO', columns='tcode', values='ep_ppvs')

plt.figure(figsize=(12, 10))
sns.set_theme(style="white")

sns.heatmap(heatmap_data,
            annot=True,
            fmt=".2f",
            cmap="RdYlGn",
            linewidths=.5,
            cbar_kws={'label': 'Índice de equitabilidad (Pielou)'})

plt.title('Evolución de la diversidad económica por estado PPVS (2003-2023)', fontsize=15)
plt.xlabel('Año censal', fontsize=12)
plt.ylabel('Entidad federativa', fontsize=12)

plt.show()

df['compot'] = np.log(
    np.clip(
        np.where(
            (df['UE'] != 0) & (df['pot_i'] != 0) & (df['UE_j'] != 0),
            (df['pot'] / df['UE']) / (df['pot_i'] / df['UE_j']),
            1e-10
        ),
        1e-10,
        None
    )
) ** 2

df['compacd'] = np.log(
    np.clip(
        np.where(
            (df['UE'] != 0) & (df['pacd_i'] != 0) & (df['UE_j'] != 0),
            (df['pacd'] / df['UE']) / (df['pacd_i'] / df['UE_j']),
            1e-10
        ),
        1e-10,
        None
    )
) ** 2

df['comppvs'] = np.log(
    np.clip(
        np.where(
            (df['UE'] != 0) & (df['ppvs_i'] != 0) & (df['UE_j'] != 0),
            (df['ppvs'] / df['UE']) / (df['ppvs_i'] / df['UE_j']),
            1e-10
        ),
        1e-10,
        None
    )
) ** 2

# Índice de brecha laboral (ibl)
df['ibl_pot'] = df['ps_pot'] - df['ps_vacb']
df['ibl_pacd'] = df['ps_pacd'] - df['ps_vacb']
df['ibl_ppvs'] = df['ps_ppvs'] - df['ps_vacb']

# Índice de absorción de capacidades tecnológicas

# Cálculo de los componentes
df['dotacion_tech'] = safe_divide(df['ataf'], df['pot'])
df['tasa_absorcion'] = safe_divide(df['itot'], df['vacb'])

# Índice Final (IACT)
# Usamos raíz cuadrada para suavizar valores extremos
df['iact'] = np.sqrt(df['dotacion_tech'] * df['tasa_absorcion'])

# Normalización (para comparar entre 0 y 1 por año)
df['iact_norm'] = df.groupby('tcode')['iact'].transform(
    lambda x: (x - x.min()) / (x.max() - x.min())
)

plt.figure(figsize=(10, 6))
sns.boxplot(x='tcode', y='iact_norm', data=df, palette='viridis')
plt.title('Distribución de la absorción tecnológica por año')
plt.ylabel('IACT normalizado')
plt.show()

def clasificar_tech(valor):
    if valor >= 0.7: return 'Vanguardia'
    if valor >= 0.3: return 'Transición'
    return 'Base/Tradicional'

df['perfil_tech'] = df['iact_norm'].apply(clasificar_tech)

# Contar cuántos sectores hay en cada categoría por año
print(df.groupby(['tcode', 'perfil_tech']).size())

# 1. Filtramos los dos años de interés
df_comparativa = df[df['tcode'].isin([2018, 2023])].copy()

# 2. Obtenemos los 10 más altos de cada año
top_10_2018 = df_comparativa[df_comparativa['tcode'] == 2018].nlargest(10, 'iact')
top_10_2023 = df_comparativa[df_comparativa['tcode'] == 2023].nlargest(10, 'iact')

# 3. Concatenamos para comparar
comparativa_tech = pd.concat([top_10_2018, top_10_2023])

# 4. Mostramos columnas clave para entender el porqué del índice
columnas_analisis = ['tcode', 'NOMGEO', 'AE', 'iact', 'ataf', 'itot', 'pot', 'vacb']
print(comparativa_tech[columnas_analisis])

lider_2023 = df[(df['tcode'] == 2023) & (df['perfil_tech'] == 'Vanguardia')]
print(lider_2023[['NOMGEO', 'AE', 'iact_norm', 'itot']])

lider_2018 = df[(df['tcode'] == 2018) & (df['perfil_tech'] == 'Vanguardia')]
print(lider_2018[['NOMGEO', 'AE', 'iact_norm', 'itot']])

lider_2013 = df[(df['tcode'] == 2013) & (df['perfil_tech'] == 'Vanguardia')]
print(lider_2013[['NOMGEO', 'AE', 'iact_norm', 'itot']])

lider_2008 = df[(df['tcode'] == 2008) & (df['perfil_tech'] == 'Vanguardia')]
print(lider_2008[['NOMGEO', 'AE', 'iact_norm', 'itot']])

lider_2003 = df[(df['tcode'] == 2003) & (df['perfil_tech'] == 'Vanguardia')]
print(lider_2003[['NOMGEO', 'AE', 'iact_norm', 'itot']])

"""Índices de productividad"""

def calcular_productividad_relativa(df):
    """
    Calcula productividad normalizada POR AÑO CENSAL.
    Elimina efecto inflacionario al comparar dentro del mismo período.
    """

    # 1. PRODUCTIVIDAD LABORAL NOMINAL (por row)
    df['prod_laboral_nom'] = safe_divide(df['vacb'], df['pot'])

    # 2. NORMALIZAR DENTRO DE CADA AÑO (z-score o min-max)
    # Esto elimina el efecto del nivel de precios del año
    df['prod_laboral_norm'] = df.groupby('tcode')['prod_laboral_nom'].transform(
        lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0
    )

    # 3. RANKING POR AÑO (más robusto que valores absolutos)
    df['rank_productividad'] = df.groupby('tcode')['prod_laboral_nom'].rank(
        ascending=False, method='average'
    )

    # 4. PRODUCTIVIDAD RELATIVA AL PROMEDIO NACIONAL DEL AÑO
    df['prod_laboral_rel'] = df['prod_laboral_nom'] / df.groupby('tcode')['prod_laboral_nom'].transform('mean')

    return df

# APLICAR
df = calcular_productividad_relativa(df)

# EJEMPLO DE USO VÁLIDO
# ✅ CORRECTO: Comparar ranking de sectores dentro de 2023
top_2023 = df[df['tcode'] == 2023].nsmallest(10, 'rank_productividad')

# ✅ CORRECTO: Comparar si un estado mejoró su posición relativa
posicion_relativa = df.groupby(['NOMGEO', 'tcode'])['rank_productividad'].mean().unstack()
mejora = posicion_relativa[2023] - posicion_relativa[2003]

# ❌ INCORRECTO: Comparar valores absolutos entre años
# df[df['tcode'] == 2023]['prod_laboral_nom'].mean() vs 2003

posicion_relativa

mejora

df['prod_cap'] = safe_divide(df['vacb'], df['ataf'])

# DICCIONARIO DE PESOS POR SECTOR (SCIAN 2 dígitos)
PESOS_SECTORIALES = {
    # Agricultura, ganadería, etc.
    '11': {'alpha': 0.70, 'beta': 0.30},
    # Minería
    '21': {'alpha': 0.40, 'beta': 0.60},
    # Manufactura
    '31-33': {'alpha': 0.65, 'beta': 0.35},
    # Construcción
    '23': {'alpha': 0.60, 'beta': 0.40},
    # Comercio
    '46-47': {'alpha': 0.75, 'beta': 0.25},
    # Servicios
    '51-56': {'alpha': 0.80, 'beta': 0.20},
    # Educación, salud
    '61-62': {'alpha': 0.85, 'beta': 0.15},
    # Otros servicios
    '71-81': {'alpha': 0.75, 'beta': 0.25},
}

# APLICAR PESOS POR SECTOR
def asignar_pesos(row):
    ae = str(row['AE'])[:2]  # Primeros 2 dígitos del sector
    for key, pesos in PESOS_SECTORIALES.items():
        if ae in key or key in ae:
            return pd.Series([pesos['alpha'], pesos['beta']])
    return pd.Series([0.70, 0.30])  # Default

df[['alpha', 'beta']] = df.apply(asignar_pesos, axis=1)

# PTF CON PESOS VARIABLES
df['ptf'] = safe_divide(df['vacb'], ((df['pot'] ** df['alpha']) * (df['ataf'] ** df['beta'])))

# PTF NORMALIZADA POR AÑO (para comparabilidad)
df['ptf_norm'] = df.groupby('tcode')['ptf'].transform(
    lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else 0
)

def crear_tablas_ptf(df, metrica='ptf', anos=None, exportar=True):
    """
    Genera batería completa de tablas dinámicas para análisis de PTF.

    Parámetros:
    -----------
    df : DataFrame
        DataFrame con datos censales y PTF calculada
    metrica : str
        Métrica a analizar ('ptf', 'ptf_norm', 'vacb', etc.)
    anos : list
        Años a incluir (None = todos)
    exportar : bool
        Si True, exporta a Excel

    Retorna:
    --------
    dict con todas las tablas dinámicas generadas
    """

    # FILTRAR AÑOS SI SE ESPECIFICA
    if anos:
        df = df[df['tcode'].isin(anos)]

    tablas = {}

    # 1. POR ENTIDAD Y AÑO
    tablas['entidad_año'] = df.pivot_table(
        values=metrica,
        index='NOMGEO',
        columns='tcode',
        aggfunc='mean',
        margins=True,
        margins_name='Promedio Nacional'
    )

    # 2. POR SECTOR Y AÑO
    tablas['sector_año'] = df.pivot_table(
        values=metrica,
        index='AE',
        columns='tcode',
        aggfunc=['mean', 'median'],
        margins=True
    )

    # 3. POR ENTIDAD Y SECTOR (ÚLTIMO AÑO)
    ultimo_ano = df['tcode'].max()
    df_ultimo = df[df['tcode'] == ultimo_ano]
    tablas['entidad_sector'] = df_ultimo.pivot_table(
        values=metrica,
        index='NOMGEO',
        columns='AE',
        aggfunc='mean',
        fill_value=0
    )

    # 4. ESTADÍSTICOS COMPLETOS
    tablas['estadisticos'] = df.groupby(['NOMGEO', 'tcode'])[metrica].agg(
        ['mean', 'median', 'std', 'min', 'max', 'count']
    ).reset_index()

    # 5. CAMBIO TEMPORAL
    if len(df['tcode'].unique()) >= 2:
        unique_years = sorted(df['tcode'].unique())
        anos_extremos = [unique_years[0], unique_years[-1]]
        ptf_inicial = df[df['tcode'] == anos_extremos[0]].groupby('NOMGEO')[metrica].mean()
        ptf_final = df[df['tcode'] == anos_extremos[-1]].groupby('NOMGEO')[metrica].mean()

        tablas['cambio_temporal'] = pd.DataFrame({
            f'{metrica}_{anos_extremos[0]}': ptf_inicial,
            f'{metrica}_{anos_extremos[-1]}': ptf_final,
            'cambio_absoluto': ptf_final - ptf_inicial,
            'cambio_porcentual': ((ptf_final - ptf_inicial) / ptf_inicial) * 100
        }).reset_index().sort_values('cambio_porcentual', ascending=False)

    # EXPORTAR A EXCEL
    if exportar:
        with pd.ExcelWriter(f'analisis_ptf_{metrica}.xlsx', engine='openpyxl') as writer:
            for nombre, tabla in tablas.items():
                # Recortar nombre de hoja (máx 31 caracteres)
                hoja = nombre[:31]
                tabla.to_excel(writer, sheet_name=hoja)

    return tablas

# EJECUTAR
tablas_ptf = crear_tablas_ptf(df, metrica='ptf', exportar=True)

# ACCEDER A CADA TABLA
print(tablas_ptf['entidad_año'].head())
print(tablas_ptf['cambio_temporal'].head())

import matplotlib.pyplot as plt
import seaborn as sns

# Obtener la tabla 'entidad_año' del diccionario
heatmap_data_entidad_ano = tablas_ptf['entidad_año'].copy()

# Excluir la columna 'Promedio Nacional' para la visualización del heatmap si existe
if 'Promedio Nacional' in heatmap_data_entidad_ano.columns:
    heatmap_data_entidad_ano = heatmap_data_entidad_ano.drop(columns=['Promedio Nacional'])

plt.figure(figsize=(14, 10))
sns.set_theme(style="white")

sns.heatmap(heatmap_data_entidad_ano,
            annot=True,
            fmt=".2f",
            cmap="RdYlGn",
            linewidths=.5,
            cbar_kws={'label': 'PTF Promedio'})

plt.title('PTF Promedio por Entidad y Año Censal', fontsize=16)
plt.xlabel('Año Censal', fontsize=12)
plt.ylabel('Entidad Federativa', fontsize=12)
plt.show()

def analisis_sensibilidad_pesos_ptf(df):
    """Prueba cómo cambia PTF con diferentes pesos α y β"""

    escenarios = {
        'Base (0.7, 0.3)': (0.7, 0.3),
        'Solow (0.75, 0.25)': (0.75, 0.25),
        'OECD (0.65, 0.35)': (0.65, 0.35),
        'Capital-intensivo (0.5, 0.5)': (0.5, 0.5),
        'Trabajo-intensivo (0.8, 0.2)': (0.8, 0.2)
    }

    resultados = {}
    df_2023 = df[df['tcode'] == 2023].copy()

    for nombre, (alpha, beta) in escenarios.items():
        ptf = df_2023['vacb'] / ((df_2023['pot'] ** alpha) *
                                  (df_2023['ataf'] ** beta))
        ptf_norm = (ptf - ptf.min()) / (ptf.max() - ptf.min())
        resultados[nombre] = ptf_norm

    # MATRIZ DE CORRELACIÓN ENTRE ESCENARIOS
    df_escenarios = pd.DataFrame(resultados)
    correlaciones = df_escenarios.corr()

    print("=" * 80)
    print("ANÁLISIS DE SENSIBILIDAD: PESOS PTF")
    print("=" * 80)
    print("\nCorrelaciones entre escenarios:")
    print(correlaciones.round(3))

    # RANKING DE ESTADOS (¿Cambia con diferentes pesos?)
    ranking_base = resultados['Base (0.7, 0.3)'].rank(ascending=False)
    ranking_extremo = resultados['Capital-intensivo (0.5, 0.5)'].rank(ascending=False)

    cambio_ranking = (ranking_base - ranking_extremo).abs()
    print(f"\nCambio promedio en ranking: {cambio_ranking.mean():.2f} posiciones")
    print(f"Máximo cambio: {cambio_ranking.max():.0f} posiciones")

    # VISUALIZAR
    plt.figure(figsize=(14, 8))
    for nombre, ptf_norm in resultados.items():
        plt.plot(ptf_norm.sort_values().values, label=nombre, linewidth=2)
    plt.xlabel('Estado (ordenado por PTF)', fontsize=12)
    plt.ylabel('PTF Normalizado', fontsize=12)
    plt.title('Sensibilidad de PTF a Pesos Factoriales', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('sensibilidad_ptf.png', dpi=300, bbox_inches='tight')
    plt.show()

    return correlaciones, df_escenarios

# EJECUTAR
corr_sensibilidad, df_sensibilidad = analisis_sensibilidad_pesos_ptf(df)

# TABLA DINÁMICA: MÚLTIPLES MÉTRICAS
# Assuming 'h_pot' was the intended 'sh_pot' for Shannon index
ptf_multi_metricas = df.pivot_table(
    values=['ptf', 'marpot', 'h_pot', 'compot','ibl_pot', 'iact_norm'],
    index=['NOMGEO', 'AE'],     # Filas combinadas
    columns='tcode',            # Columnas: Años
    aggfunc='mean',
    margins=True
)

print("=" * 80)
print("MÚLTIPLES MÉTRICAS POR ENTIDAD Y SECTOR")
print("=" * 80)
print(ptf_multi_metricas.round(2))

# EXPORTAR
ptf_multi_metricas.to_excel('ptf_multi_metricas.xlsx')

import seaborn as sns
import matplotlib.pyplot as plt

# PREPARAR DATOS PARA HEATMAP
ptf_heatmap = df.pivot_table(
    values='ptf_norm',
    index='NOMGEO',
    columns='tcode',
    aggfunc='mean'
)

# GRAFICAR
plt.figure(figsize=(14, 12))
sns.heatmap(
    ptf_heatmap,
    annot=True,
    fmt='.2f',
    cmap='RdYlGn',        # Rojo-Amarillo-Verde
    linewidths=0.5,
    cbar_kws={'label': 'PTF Normalizado'}
)
plt.title('Evolución de PTF por Entidad Federativa (2003-2023)', fontsize=15)
plt.xlabel('Año Censal', fontsize=12)
plt.ylabel('Entidad Federativa', fontsize=12)
plt.xticks(rotation=0)
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# GUARDAR FIGURA
plt.savefig('ptf_heatmap_entidades.png', dpi=300, bbox_inches='tight')

# PREPARAR DATOS
ptf_lineas = df.pivot_table(
    values='ptf',
    index='tcode',
    columns='AE',
    aggfunc='mean'
)

# GRAFICAR TOP 10 SECTORES
top_10_sectores = df.groupby('AE')['ptf'].mean().nlargest(10).index

plt.figure(figsize=(14, 8))
for sector in top_10_sectores:
    plt.plot(ptf_lineas.index, ptf_lineas[sector], marker='o', label=sector, linewidth=2)

plt.title('Tendencia de PTF por Sector Económico (Top 10)', fontsize=15)
plt.xlabel('Año Censal', fontsize=12)
plt.ylabel('PTF', fontsize=12)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

plt.savefig('ptf_tendencias_sectores.png', dpi=300, bbox_inches='tight')

plt.figure(figsize=(12, 6))
sns.boxplot(
    data=df,
    x='tcode',
    y='ptf_norm',
    palette='viridis',
    showfliers=False  # Ocultar outliers extremos
)
plt.title('Distribución de PTF Normalizada por Año Censal', fontsize=15)
plt.xlabel('Año Censal', fontsize=12)
plt.ylabel('PTF Normalizado', fontsize=12)
plt.grid(True, alpha=0.3, axis='y')
plt.show()

plt.savefig('ptf_boxplot_anos.png', dpi=300, bbox_inches='tight')

ptf_por_estado_2023 = df[df['tcode'] == 2023].groupby('NOMGEO')['ptf'].mean().sort_values(ascending=False).head(10)

plt.figure(figsize=(12, 7))
sns.barplot(x=ptf_por_estado_2023.values, y=ptf_por_estado_2023.index, palette='magma')
plt.title('Top 10 Estados con Mayor PTF Promedio (2023)', fontsize=15)
plt.xlabel('PTF Promedio', fontsize=12)
plt.ylabel('Estado', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()

"""Índices espaciales"""

# Índice de Concentración Geográfica

# Gran Total Nacional por año (Denominador global)
df['pot_tn'] = df.groupby('tcode')['pot'].transform('sum')
df['pacd_tn'] = df.groupby('tcode')['pacd'].transform('sum')
df['ppvs_tn'] = df.groupby('tcode')['ppvs'].transform('sum')

# Calcular los dos términos de la resta dentro del valor absoluto
# Término A: Participación del estado j en el sector i
df['term_A_pot'] = safe_divide(df['pot'], df['pot_i'])
df['term_A_pacd'] = safe_divide(df['pacd'], df['pacd_i'])
df['term_A_ppvs'] = safe_divide(df['ppvs'], df['ppvs_i'])

# Término B: Participación del estado j en la economía nacional total
df['term_B_pot'] = safe_divide(df['pot_j'], df['pot_tn'])
df['term_B_pacd'] = safe_divide(df['pacd_j'], df['pacd_tn'])
df['term_B_ppvs'] = safe_divide(df['ppvs_j'], df['ppvs_tn'])

# Calcular el valor absoluto de la diferencia
df['abs_diff_pot'] = (df['term_A_pot'] - df['term_B_pot']).abs()
df['abs_diff_pacd'] = (df['term_A_pacd'] - df['term_B_pacd']).abs()
df['abs_diff_ppvs'] = (df['term_A_ppvs'] - df['term_B_ppvs']).abs()

# Sumar sobre todos los estados (j) para cada Sector y Año, y multiplicar por 0.5
qs_df_pot = df.groupby(['tcode', 'AE'])['abs_diff_pot'].sum().reset_index()
qs_df_pacd = df.groupby(['tcode', 'AE'])['abs_diff_pacd'].sum().reset_index()
qs_df_ppvs = df.groupby(['tcode', 'AE'])['abs_diff_ppvs'].sum().reset_index()

qs_df_pot['Qs_pot'] = qs_df_pot['abs_diff_pot'] * 0.5
qs_df_pacd['Qs_pacd'] = qs_df_pacd['abs_diff_pacd'] * 0.5
qs_df_ppvs['Qs_ppvs'] = qs_df_ppvs['abs_diff_ppvs'] * 0.5

# Índice de Especialización Regional ($Qr$)

# Cálculo de los componentes
# Participación local del sector i en el estado j
df['pl_pot'] = safe_divide(df['pot'], df['pot_j'])
df['pl_pacd'] = safe_divide(df['pacd'], df['pacd_j'])
df['pl_ppvs'] = safe_divide(df['ppvs'], df['ppvs_j'])

# Participación nacional del sector i
df['pn_pot'] = safe_divide(df['pot_i'], df['pot_tn'])
df['pn_pacd'] = safe_divide(df['pacd_i'], df['pacd_tn'])
df['pn_ppvs'] = safe_divide(df['ppvs_i'], df['ppvs_tn'])

# Cálculo del Índice Qr (Agrupado por Estado y Año)
qr_df_pot = df.groupby(['tcode', 'NOMGEO']).apply(
    lambda x: 0.5 * np.sum(np.abs(x['pl_pot'] - x['pn_pot']))
).reset_index(name='Qr_pot')
qr_df_pacd = df.groupby(['tcode', 'NOMGEO']).apply(
    lambda x: 0.5 * np.sum(np.abs(x['pl_pacd'] - x['pn_pacd']))
).reset_index(name='Qr_pacd')
qr_df_ppvs = df.groupby(['tcode', 'NOMGEO']).apply(
    lambda x: 0.5 * np.sum(np.abs(x['pl_ppvs'] - x['pn_ppvs']))
).reset_index(name='Qr_ppvs')

# Merge Qr_pot into the main DataFrame df
df = pd.merge(df, qr_df_pot[['tcode', 'NOMGEO', 'Qr_pot']], on=['tcode', 'NOMGEO'], how='left')

crecimiento_pot = df.groupby(['tcode', 'NOMGEO'])['pot'].sum().reset_index()
crecimiento_pacd = df.groupby(['tcode', 'NOMGEO'])['pacd'].sum().reset_index()
crecimiento_ppvs = df.groupby(['tcode', 'NOMGEO'])['ppvs'].sum().reset_index()

crecimiento_pivot_pot = crecimiento_pot.pivot(index=['NOMGEO'], columns='tcode', values='pot')
crecimiento_pivot_pacd = crecimiento_pacd.pivot(index=['NOMGEO'], columns='tcode', values='pacd')
crecimiento_pivot_ppvs = crecimiento_ppvs.pivot(index=['NOMGEO'], columns='tcode', values='ppvs')

crecimiento_pivot_pot['pct_crecimiento_pot'] = (crecimiento_pivot_pot[2023] - crecimiento_pivot_pot[2003]) / crecimiento_pivot_pot[2003] * 100
crecimiento_pivot_pacd['pct_crecimiento_pacd'] = (crecimiento_pivot_pacd[2023] - crecimiento_pivot_pacd[2003]) / crecimiento_pivot_pacd[2003] * 100
crecimiento_pivot_ppvs['pct_crecimiento_ppvs'] = (crecimiento_pivot_ppvs[2023] - crecimiento_pivot_ppvs[2003]) / crecimiento_pivot_ppvs[2003] * 100

qrpot_2023 = qr_df_pot[qr_df_pot['tcode'] == 2023].set_index(['NOMGEO'])
datapot_plot = qrpot_2023.join(crecimiento_pivot_pot['pct_crecimiento_pot']).reset_index()
qrpacd_2023 = qr_df_pacd[qr_df_pacd['tcode'] == 2023].set_index(['NOMGEO'])
datapacd_plot = qrpacd_2023.join(crecimiento_pivot_pacd['pct_crecimiento_pacd']).reset_index()
qrppvs_2023 = qr_df_ppvs[qr_df_ppvs['tcode'] == 2023].set_index(['NOMGEO'])
datappvs_plot = qrppvs_2023.join(crecimiento_pivot_ppvs['pct_crecimiento_ppvs']).reset_index()

plt.figure(figsize=(12, 8))
sns.set_theme(style="whitegrid")

scatter = sns.scatterplot(
    data=datapot_plot,
    x='Qr_pot',
    y='pct_crecimiento_pot',
    size=2023, # El tamaño del punto puede ser el total de personal en 2023
    sizes=(50, 500),
    alpha=0.7,
    hue='Qr_pot', # El color cambia según la especialización
    palette='viridis'
)

# Añadir etiquetas a los puntos (solo para los más destacados)
for i in range(datapot_plot.shape[0]):
    # Etiquetar estados con Qr alto o crecimiento muy alto/bajo
    if datapot_plot.Qr_pot[i] > 0.22 or datapot_plot.pct_crecimiento_pot[i] > 100:
        plt.text(
            datapot_plot.Qr_pot[i]+0.005,
            datapot_plot.pct_crecimiento_pot[i],
            datapot_plot.NOMGEO[i],
            fontsize=9
        )

plt.title('Relación entre especialización económica ($Qr$) y crecimiento laboral (2003-2023)', fontsize=15)
plt.xlabel('Índice de especialización regional ($Qr$) - Año 2023', fontsize=12)
plt.ylabel('Crecimiento porcentual del personal (%)', fontsize=12)
plt.axvline(datapot_plot['Qr_pot'].mean(), color='red', linestyle='--', alpha=0.5, label='Media Qr_pot')

plt.show()

plt.figure(figsize=(12, 8))
sns.set_theme(style="whitegrid")

scatter = sns.scatterplot(
    data=datapacd_plot,
    x='Qr_pacd',
    y='pct_crecimiento_pacd',
    size=2023, # El tamaño del punto puede ser el total de personal en 2023
    sizes=(50, 500),
    alpha=0.7,
    hue='Qr_pacd', # El color cambia según la especialización
    palette='viridis'
)

# Añadir etiquetas a los puntos (solo para los más destacados)
for i in range(datapacd_plot.shape[0]):
    # Etiquetar estados con Qr alto o crecimiento muy alto/bajo
    if datapacd_plot.Qr_pacd[i] > 0.22 or datapacd_plot.pct_crecimiento_pacd[i] > 100:
        plt.text(
            datapacd_plot.Qr_pacd[i]+0.005,
            datapacd_plot.pct_crecimiento_pacd[i],
            datapacd_plot.NOMGEO[i],
            fontsize=9
        )

plt.title('Relación entre especialización económica ($Qr$) y crecimiento laboral PACD (2003-2023)', fontsize=15)
plt.xlabel('Índice de especialización regional ($Qr$) - Año 2023', fontsize=12)
plt.ylabel('Crecimiento porcentual del PACD (%)', fontsize=12)
plt.axvline(datapacd_plot['Qr_pacd'].mean(), color='red', linestyle='--', alpha=0.5, label='Media Qr_pacd')

plt.show()

plt.figure(figsize=(12, 8))
sns.set_theme(style="whitegrid")

scatter = sns.scatterplot(
    data=datappvs_plot,
    x='Qr_ppvs',
    y='pct_crecimiento_ppvs',
    size=2023, # El tamaño del punto puede ser el total de personal en 2023
    sizes=(50, 500),
    alpha=0.7,
    hue='Qr_ppvs', # El color cambia según la especialización
    palette='viridis'
)

# Añadir etiquetas a los puntos (solo para los más destacados)
for i in range(datappvs_plot.shape[0]):
    # Etiquetar estados con Qr alto o crecimiento muy alto/bajo
    if datappvs_plot.Qr_ppvs[i] > 0.22 or datappvs_plot.pct_crecimiento_ppvs[i] > 100:
        plt.text(
            datappvs_plot.Qr_ppvs[i]+0.005,
            datappvs_plot.pct_crecimiento_ppvs[i],
            datappvs_plot.NOMGEO[i],
            fontsize=9
        )

plt.title('Relación entre especialización económica ($Qr$) y crecimiento laboral PPVS (2003-2023)', fontsize=15)
plt.xlabel('Índice de especialización regional ($Qr$) - Año 2023', fontsize=12)
plt.ylabel('Crecimiento porcentual del PPVS (%)', fontsize=12)
plt.axvline(datappvs_plot['Qr_ppvs'].mean(), color='red', linestyle='--', alpha=0.5, label='Media Qr_ppvs')

plt.show()

df_2003 = df[df['tcode'] == 2003].copy()
df_2023 = df[df['tcode'] == 2023].copy()

# Renombramos columnas para identificar V(0) y V(T)
# Usaremos 'AE' como sector y 'NOMGEO' como estado para el cruce (Merge)
df_base = df_2003[['NOMGEO', 'AE', 'pot']].rename(columns={'pot': 'pot_0'})
df_actual = df_2023[['NOMGEO', 'AE', 'pot']].rename(columns={'pot': 'pot_T'})

# Unimos ambos años en un solo DataFrame de trabajo
df_dinamico = pd.merge(df_actual, df_base, on=['NOMGEO', 'AE'], how='left')

df_dinamico['rv_ij'] = safe_divide(df_dinamico['pot_T'], df_dinamico['pot_0'])

df_dinamico['total_estado_T'] = df_dinamico.groupby('NOMGEO')['pot_T'].transform('sum')
df_dinamico['total_estado_0'] = df_dinamico.groupby('NOMGEO')['pot_0'].transform('sum')

# Calculamos las proporciones V_ij / sum(V_ij)
df_dinamico['prop_T'] = safe_divide(df_dinamico['pot_T'], df_dinamico['total_estado_T'])
df_dinamico['prop_0'] = safe_divide(df_dinamico['pot_0'], df_dinamico['total_estado_0'])

# Calculamos el valor absoluto de la diferencia
df_dinamico['diff_abs_crr'] = (df_dinamico['prop_T'] - df_dinamico['prop_0']).abs()

# Sumamos por estado y multiplicamos por 1/2
crr_resultado = df_dinamico.groupby(['NOMGEO'])['diff_abs_crr'].sum().reset_index()
crr_resultado['CRr'] = crr_resultado['diff_abs_crr'] * 0.5

plt.figure(figsize=(10, 12))
sns.barplot(data=crr_resultado.sort_values('CRr', ascending=False), x='CRr', y='NOMGEO', palette='magma')
plt.title('Coeficiente de reestructuración estatal (2004-2023)')
plt.axvline(0.5, color='red', linestyle='--') # Umbral de cambio profundo
plt.show()

df_dinamico['total_nacional_sec_T'] = df_dinamico.groupby('AE')['pot_T'].transform('sum')
df_dinamico['total_nacional_sec_0'] = df_dinamico.groupby('AE')['pot_0'].transform('sum')

# Calculamos las participaciones (proporciones geográficas)
# Si el total nacional es 0 (sector extinto), el resultado será 0
df_dinamico['part_geo_T'] = safe_divide(df_dinamico['pot_T'], df_dinamico['total_nacional_sec_T'])
df_dinamico['part_geo_0'] = safe_divide(df_dinamico['pot_0'], df_dinamico['total_nacional_sec_0'])

# Calculamos la diferencia absoluta
df_dinamico['diff_abs_crs'] = (df_dinamico['part_geo_T'] - df_dinamico['part_geo_0']).abs()

# Agrupamos por SECTOR (AE) para obtener el CRs final
crs_resultado = df_dinamico.groupby('AE')['diff_abs_crs'].sum().reset_index()
crs_resultado['CRs'] = crs_resultado['diff_abs_crs'] * 0.5

crs_plot = crs_resultado.sort_values('CRs', ascending=False).head(20) # Top 20 para no saturar

plt.figure(figsize=(12, 8))
sns.set_theme(style="whitegrid")

sns.barplot(
    data=crs_plot,
    x='CRs',
    y='AE',
    palette='flare' # Degradado de color para enfatizar el cambio
)

plt.axvline(crs_resultado['CRs'].mean(), color='blue', linestyle='--', label='Promedio Nacional')

plt.title('Reestructuración geográfica sectorial (2004-2023)', fontsize=15)
plt.xlabel('Coeficiente de reestructuración sectorial (CRs)', fontsize=12)
plt.ylabel('Código de sector', fontsize=12)
plt.legend()

plt.tight_layout()
plt.show()

sector_top = crs_plot.iloc[0]['AE']
detalle = df_dinamico[df_dinamico['AE'] == sector_top][['NOMGEO', 'part_geo_0', 'part_geo_T']]

# Graficar los 5 estados que más ganaron y los 5 que más perdieron participación
detalle['cambio'] = detalle['part_geo_T'] - detalle['part_geo_0']
detalle_cambio = detalle.sort_values('cambio', ascending=False)

print(f"Cambios geográficos para el sector: {sector_top}")
print(pd.concat([detalle_cambio.head(5), detalle_cambio.tail(5)]))

# Calculamos el crecimiento nacional por sector (rVi)
crecimiento_sectorial = df_dinamico.groupby('AE').agg({
    'pot_T': 'sum',
    'pot_0': 'sum'
}).reset_index()

crecimiento_sectorial['rVi'] = safe_divide(crecimiento_sectorial['pot_T'], crecimiento_sectorial['pot_0'])

# Unimos con el CRs que ya teníamos
cuadrante_data = pd.merge(crs_resultado[['AE', 'CRs']], crecimiento_sectorial[['AE', 'rVi']], on='AE')

# Limpiamos valores infinitos o NaN por si algún sector era nuevo
cuadrante_data = cuadrante_data.replace([np.inf, -np.inf], np.nan).dropna()

plt.figure(figsize=(12, 10))

# Definir los umbrales
x_med = cuadrante_data['CRs'].median()
y_med = 1.0 # 1.0 significa crecimiento cero; arriba es crecimiento, abajo es pérdida

# Crear el Scatter Plot
sns.scatterplot(data=cuadrante_data, x='CRs', y='rVi', s=100, alpha=0.6, color='darkblue')

# Añadir las líneas del cuadrante
plt.axvline(x_med, color='red', linestyle='--', alpha=0.5)
plt.axhline(y_med, color='red', linestyle='--', alpha=0.5)

# Etiquetas para los cuadrantes
plt.text(cuadrante_data['CRs'].max()*0.8, cuadrante_data['rVi'].max()*0.9, 'ESTRELLA (Dinámicos)', fontsize=12, color='green', fontweight='bold')
plt.text(cuadrante_data['CRs'].min(), cuadrante_data['rVi'].max()*0.9, 'MADUROS (Crecientes estables)', fontsize=12, color='blue', fontweight='bold')
plt.text(cuadrante_data['CRs'].min(), cuadrante_data['rVi'].min(), 'ESTÁTICOS (En declive)', fontsize=12, color='gray', fontweight='bold')
plt.text(cuadrante_data['CRs'].max()*0.8, cuadrante_data['rVi'].min(), 'TRANSICIÓN (Reubicación crítica)', fontsize=12, color='orange', fontweight='bold')

# Etiquetar algunos puntos clave (Sectores más extremos)
top_sectors = pd.concat([cuadrante_data.nlargest(5, 'rVi'), cuadrante_data.nlargest(5, 'CRs')])
for i, row in top_sectors.iterrows():
    plt.text(row['CRs']+0.01, row['rVi'], row['AE'], fontsize=9)

plt.title('Cuadrante de dinamismo sectorial (2004-2023)', fontsize=15)
plt.xlabel('Reestructuración geográfica (CRs)', fontsize=12)
plt.ylabel('Crecimiento relativo nacional (rVi)', fontsize=12)
plt.yscale('log') # Usamos escala logarítmica si hay crecimientos muy disparados

plt.show()

# Definimos los umbrales
umbral_crs = cuadrante_data['CRs'].median()
umbral_crecimiento = 1.0 # Crecimiento positivo

# Filtramos el cuadrante superior derecho
estrellas = cuadrante_data[
    (cuadrante_data['CRs'] > umbral_crs) &
    (cuadrante_data['rVi'] > umbral_crecimiento)
].copy()

# Ordenamos por dinamismo (una combinación de ambos factores)
estrellas['puntuacion_dinamismo'] = estrellas['CRs'] * estrellas['rVi']
estrellas_ranking = estrellas.sort_values('puntuacion_dinamismo', ascending=False)

# Mostrar el Top 10 de sectores Estrella
print("Top 10 Sectores Estrella (Dinámicos y en Reubicación):")
print(estrellas_ranking[['AE', 'CRs', 'rVi']].head(10))

# Tomamos el sector líder de las estrellas
sector_lider = estrellas_ranking.iloc[0]['AE']

# Buscamos en qué estados creció más su participación
ganadores = df_dinamico[df_dinamico['AE'] == sector_lider].copy()
ganadores['cambio_participacion'] = ganadores['part_geo_T'] - ganadores['part_geo_0']

print(f"\nEstados que 'conquistaron' el sector {sector_lider}:")
print(ganadores.sort_values('cambio_participacion', ascending=False)[['NOMGEO', 'cambio_participacion']].head(5))

# Calcular rSi: Tasa de crecimiento nacional por sector (i)
# Usamos los totales nacionales que calculamos para el CRs
crecimiento_nacional_it = df_dinamico.groupby('AE')['pot_T'].sum()
crecimiento_nacional_i0 = df_dinamico.groupby('AE')['pot_0'].sum()

# rSi = V_i(t) / V_i(0)
rsi = (crecimiento_nacional_it / crecimiento_nacional_i0).rename('rsi')

# Unir rSi al dataframe dinámico
df_shift = pd.merge(df_dinamico, rsi, on='AE', how='left')

# Calcular el Valor Esperado (lo que se supone que debía crecer según la nación)
# Valor_esperado = V_ij(0) * rSi
df_shift['valor_esperado'] = df_shift['pot_0'] * df_shift['rsi']

# Calcular el Efecto Diferencial (EDj) por estado
# Es la suma de (Lo observado - Lo esperado)
df_shift['diff_local'] = df_shift['pot_T'] - df_shift['valor_esperado']

edj_resultado = df_shift.groupby(['NOMGEO'])['diff_local'].sum().reset_index()
edj_resultado.rename(columns={'diff_local': 'EDj'}, inplace=True)

plt.figure(figsize=(12, 10))
# Ordenamos para ver quién tiene mayor ventaja competitiva
edj_sorted = edj_resultado.sort_values('EDj', ascending=False)

sns.barplot(data=edj_sorted, x='EDj', y='NOMGEO',
            palette=['green' if x > 0 else 'red' for x in edj_sorted['EDj']])

plt.axvline(0, color='black', linestyle='-')
plt.title('Efecto diferencial (EDj): Ventaja competitiva por Estado (2004-2023)', fontsize=15)
plt.xlabel('Empleos generados por encima/debajo de la tendencia nacional', fontsize=12)
plt.show()

# Promediamos el IACT normalizado por estado para el 2023
iact_estatal = df[df['tcode'] == 2023].groupby(['NOMGEO'])['iact_norm'].mean().reset_index()

# 2. Unimos con el Efecto Diferencial (EDj)
analisis_final = pd.merge(edj_resultado, iact_estatal, on=['NOMGEO'])

# 3. Calculamos la correlación para ver qué tan fuerte es la relación
correlacion = analisis_final['EDj'].corr(analisis_final['iact_norm'])
print(f"La correlación entre tecnología y competitividad es: {correlacion:.2f}")

plt.figure(figsize=(14, 9))
sns.set_theme(style="whitegrid")

# Definir los umbrales de los cuadrantes
x_med = analisis_final['iact_norm'].mean()
y_med = 0  # El EDj es 0 para el equilibrio de competitividad

# Crear el scatter plot base
scatter = sns.scatterplot(
    data=analisis_final,
    x='iact_norm', y='EDj',
    size='iact_norm', hue='EDj',
    palette='RdYlGn', sizes=(100, 1000), alpha=0.6
)

# Función para filtrar y etiquetar el Top 3 por cuadrante
def etiquetar_top_cuadrantes(df, x_col, y_col, name_col, x_m, y_m):
    # Cuadrante 1: Alto IACT, Alto EDj (Líderes)
    c1 = df[(df[x_col] >= x_m) & (df[y_col] >= y_m)].nlargest(3, y_col)
    # Cuadrante 2: Bajo IACT, Alto EDj (Eficientes)
    c2 = df[(df[x_col] < x_m) & (df[y_col] >= y_m)].nlargest(3, y_col)
    # Cuadrante 3: Bajo IACT, Bajo EDj (Rezagados)
    c3 = df[(df[x_col] < x_m) & (df[y_col] < y_m)].nsmallest(3, y_col)
    # Cuadrante 4: Alto IACT, Bajo EDj (Falla de Absorción)
    c4 = df[(df[x_col] >= x_m) & (df[y_col] < y_m)].nsmallest(3, y_col)

    tops = pd.concat([c1, c2, c3, c4])

    for i, row in tops.iterrows():
        plt.text(row[x_col] + 0.005, row[y_col], row[name_col],
                 fontsize=11, fontweight='bold', alpha=0.9)

# Aplicar la función de etiquetado
etiquetar_top_cuadrantes(analisis_final, 'iact_norm', 'EDj', 'NOMGEO', x_med, y_med)

# Líneas y estética
plt.axhline(y_med, color='black', linestyle='--', alpha=0.4)
plt.axvline(x_med, color='blue', linestyle='--', alpha=0.4)
plt.title('Top 3 Estados por Perfil Tecnológico y Competitivo', fontsize=16)

plt.show()

def asignar_perfil(row, xm, ym):
    if row['iact_norm'] >= xm and row['EDj'] >= ym: return 'Líder Tecnológico'
    if row['iact_norm'] < xm and row['EDj'] >= ym: return 'Eficiente Tradicional'
    if row['iact_norm'] < xm and row['EDj'] < ym: return 'Rezagado'
    return 'Falla de Absorción'

analisis_final['Perfil_Estratégico'] = analisis_final.apply(asignar_perfil, args=(x_med, y_med), axis=1)

# Ver cuántos estados hay en cada categoría
resumen_perfiles = analisis_final['Perfil_Estratégico'].value_counts()
print(resumen_perfiles)

# Calcular la tasa de crecimiento nacional global (R_nacional)
total_t = df_dinamico['pot_T'].sum()
total_0 = df_dinamico['pot_0'].sum()
R_nacional = total_t / total_0

# Aplicar la fórmula del Efecto Estructural (EEj)
# EEj = suma de { V_ij(0) * (rSi - R_nacional) }
df_shift['efecto_estructural_sectorial'] = df_shift['pot_0'] * (df_shift['rsi'] - R_nacional)

eej_resultado = df_shift.groupby(['NOMGEO'])['efecto_estructural_sectorial'].sum().reset_index()
eej_resultado.rename(columns={'efecto_estructural_sectorial': 'EEj'}, inplace=True)

# Unir todos los componentes en una tabla maestra
resumen_shift_share = pd.merge(edj_resultado, eej_resultado, on=['NOMGEO'])

# Añadir el crecimiento total observado para comparar
crecimiento_total = df_dinamico.groupby(['NOMGEO'])['pot_T'].sum() - \
                   df_dinamico.groupby(['NOMGEO'])['pot_0'].sum()
resumen_shift_share['Crecimiento_Total'] = crecimiento_total.values

resumen_shift_share.set_index('NOMGEO')[['EEj', 'EDj']].plot(kind='bar', stacked=True, figsize=(15,7))
plt.title('Descomposición del Crecimiento Estatal: Estructura vs. Competitividad')
plt.ylabel('Personal Ocupado')
plt.axhline(0, color='black')
plt.show()

# Calcular inversos de tasas de crecimiento (T0 / Tt)
# Inverso Nacional
inverso_R_nacional = df_dinamico['pot_0'].sum() / df_dinamico['pot_T'].sum()

# Inverso Estatal (por cada estado)
totales_estatales = df_dinamico.groupby(['NOMGEO']).agg({
    'pot_T': 'sum',
    'pot_0': 'sum'
}).reset_index()

totales_estatales['inverso_R_estatal'] = totales_estatales['pot_0'] / totales_estatales['pot_T']

# 2. Unir al dataframe principal
df_shift = pd.merge(df_shift, totales_estatales[['NOMGEO', 'inverso_R_estatal']], on='NOMGEO', how='left')

# 3. Aplicar la fórmula del Efecto Inercial por sector
# EI = V(t) * (Inverso_Nac - Inverso_Est)
df_shift['ei_sectorial'] = df_shift['pot_T'] * (inverso_R_nacional - df_shift['inverso_R_estatal'])

# 4. Agrupar por estado para obtener el EIj final
eij_resultado = df_shift.groupby(['NOMGEO'])['ei_sectorial'].sum().reset_index()
eij_resultado.rename(columns={'ei_sectorial': 'EIj'}, inplace=True)

eij_plot = eij_resultado.sort_values('EIj', ascending=False)

# Crear el gráfico
plt.figure(figsize=(10, 12))
colors = ['#2ecc71' if x > 0 else '#e74c3c' for x in eij_plot['EIj']]

sns.barplot(
    data=eij_plot,
    x='EIj',
    y='NOMGEO',
    palette=colors
)

# 3. Estética y referencias
plt.axvline(0, color='black', linewidth=1.5, linestyle='-')
plt.title('Efecto Inercial ($EI_j$): Inercia del Crecimiento Estatal vs. Nacional\n(2004-2023)', fontsize=14)
plt.xlabel('Magnitud del Efecto Inercial (Positivo = Ritmo superior al Nacional)', fontsize=12)
plt.ylabel('Entidad Federativa', fontsize=12)
plt.grid(axis='x', linestyle='--', alpha=0.6)

plt.tight_layout()
plt.show()

# Preparar datos: Crecimiento Nacional (CN), Efecto Estructural (EEj) y Diferencial (EDj)
# Nota: CN = V_ij(0) * (R_nacional - 1)
resumen_shift_share['CN'] = df_dinamico.groupby(['NOMGEO'])['pot_0'].sum().values * (R_nacional - 1)

plot_data = resumen_shift_share.set_index('NOMGEO')[['CN', 'EEj', 'EDj']]

# Graficar
ax = plot_data.sort_values('CN', ascending=True).plot(
    kind='barh',
    stacked=True,
    figsize=(12, 10),
    color=['#3498db', '#f1c40f', '#2ecc71']
)

plt.axvline(0, color='black', linewidth=0.8)
plt.title('Componentes del Cambio en el Personal Ocupado (2004-2023)', fontsize=15)
plt.xlabel('Número de Personas Ocupadas')
plt.legend(['Crecimiento Nacional (Inercia)', 'Efecto Estructural (Mezcla)', 'Efecto Diferencial (Competitividad)'])
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 8))
sns.scatterplot(data=resumen_shift_share, x='EEj', y='EDj', s=100, color='red')

# Líneas cruzadas en el origen (0,0)
plt.axhline(0, color='black', linestyle='--', alpha=0.5)
plt.axvline(0, color='black', linestyle='--', alpha=0.5)

# Etiquetas de cuadrantes
plt.text(resumen_shift_share['EEj'].max()*0.5, resumen_shift_share['EDj'].max()*0.8, 'GANADORES\n(Sectores dinámicos + Eficiencia)', color='green', fontweight='bold')
plt.text(resumen_shift_share['EEj'].min()*0.8, resumen_shift_share['EDj'].max()*0.8, 'SOBREVIVIENTES\n(Sectores lentos + Eficiencia)', color='blue', fontweight='bold')
plt.text(resumen_shift_share['EEj'].min()*0.8, resumen_shift_share['EDj'].min()*0.8, 'REZAGADOS\n(Sectores lentos + Ineficiencia)', color='red', fontweight='bold')

# Etiquetar solo los 5 estados más extremos
for i, row in resumen_shift_share.nlargest(5, 'EDj').iterrows():
    plt.text(row['EEj'], row['EDj'], row['NOMGEO'])

plt.title('Matriz de Diagnóstico Regional: Estructura vs. Competitividad', fontsize=14)
plt.xlabel('Efecto Estructural (EEj)')
plt.ylabel('Efecto Diferencial (EDj)')
plt.show()

def veredicto(row):
    if row['EEj'] > 0 and row['EDj'] > 0: return "Líder: Sectores ganadores y alta competitividad."
    if row['EEj'] < 0 and row['EDj'] > 0: return "Resiliente: Supera su mala estructura sectorial."
    if row['EEj'] > 0 and row['EDj'] < 0: return "Desaprovechado: Tiene buenos sectores pero es ineficiente."
    return "Crítico: Estructura obsoleta y baja competitividad."

resumen_shift_share['Conclusión'] = resumen_shift_share.apply(veredicto, axis=1)

# Mostrar los estados más interesantes
print(resumen_shift_share[['NOMGEO', 'EEj', 'EDj', 'Conclusión']].sort_values('EDj', ascending=False))

# Calcular el promedio de automatización por sector (AE) en 2023
automa_sectorial = df[df['tcode'] == 2023].groupby('AE').agg({
    'atmep': 'sum',
    'ppvs': 'sum',
    'pot': 'sum'
}).reset_index()

automa_sectorial['automa'] = automa_sectorial['atmep'] / automa_sectorial['ppvs']

# Unir con la tasa de crecimiento rSi
bubble_data = pd.merge(automa_sectorial, rsi.reset_index(), on='AE')

# Limpiar valores para el gráfico (evitar división por cero o NaNs)
bubble_data = bubble_data.replace([np.inf, -np.inf], np.nan).dropna()

plt.figure(figsize=(14, 10))

# Crear el gráfico de burbujas
# Eje X: Automatización (Log para manejar escalas grandes)
# Eje Y: Crecimiento Sectorial
# Tamaño: Personal Ocupado Total (pot)
scatter = plt.scatter(
    x=bubble_data['automa'],
    y=bubble_data['rsi'],
    s=bubble_data['pot'] / 100, # Ajustar escala del tamaño
    alpha=0.5,
    c=bubble_data['rsi'],
    cmap='viridis',
    edgecolors="w",
    linewidth=1
)

# Líneas de referencia
plt.axhline(1, color='red', linestyle='--', alpha=0.5) # Línea de no crecimiento
plt.axvline(bubble_data['automa'].median(), color='blue', linestyle='--', alpha=0.5) # Mediana de automatización

# Etiquetar los 8 sectores más automatizados o con más crecimiento
top_labels = bubble_data.nlargest(8, 'automa')
for i, row in top_labels.iterrows():
    plt.text(row['automa'], row['rsi'], row['AE'], fontsize=9, fontweight='bold')

plt.xscale('log') # Escala logarítmica para ver mejor la distribución
plt.title('Grado de Automatización vs. Crecimiento Sectorial (2004-2023)', fontsize=16)
plt.xlabel('Grado de Automatización (Activos / Personal Operativo) - Escala Log', fontsize=12)
plt.ylabel('Crecimiento Nacional del Sector (rSi)', fontsize=12)
plt.colorbar(label='Tasa de Crecimiento')

plt.show()

# Merge EDj and EEj from resumen_shift_share into the main DataFrame df
# Note: EDj and EEj are calculated for the entire period (2003-2023), not year-specific.
# When merged on 'NOMGEO', these values will be broadcasted across all years and sectors for each state.
df = pd.merge(df, resumen_shift_share[['NOMGEO', 'EDj', 'EEj']], on='NOMGEO', how='left')

"""Guardado de procesos"""

output_file_name = 'df_exportado.xlsx'
df.to_excel(output_file_name, index=False)
print(f"DataFrame guardado exitosamente como '{output_file_name}'")

from google.colab import drive
drive.mount('/content/drive')

output_file_path = '/content/drive/MyDrive/df_exportado_drive.xlsx'
df.to_excel(output_file_path, index=False)
print(f"DataFrame guardado exitosamente en '{output_file_path}'")
